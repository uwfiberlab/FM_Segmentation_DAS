{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403d9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd32ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Create a ListedColormap: 0 -> transparent, 1 -> orange\n",
    "cmap_orange_transparent = mcolors.ListedColormap([(1,1,1,0), (1,0.5,0,1)])  # 0: transparent, 1: orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738cdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters for DAS \n",
    "sample_rate = 25\n",
    "dchan = 9.5714\n",
    "ch_max = 4500  # max channel of each cable (4500 or 6000)\n",
    "ch_itv=2  # channels are downsampled for faster picking\n",
    "\n",
    "### Directories and files\n",
    "raw_dir = '/fd1/QibinShi_data/akdas/qibin_data/'\n",
    "out_dir = raw_dir + 'largerEQ_plots_test_picking_dec_ch' + str(ch_max) + '/'\n",
    "record_time_file = 'recording_times_larger.csv'\n",
    "qml = raw_dir + 'ak_Dec1_31_a120b065.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942f3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read phase picks from the previous session\n",
    "with h5py.File(out_dir + 'phase_picks.hdf5', 'r') as f: #filepaths should be \n",
    "    raw_picks = f[\"raw_alldata_picks\"][:]\n",
    "    one_picks = f[\"one_denoise_picks\"][:]\n",
    "    mul_picks = f[\"mul_denoise_picks\"][:]\n",
    "    pred_picks = f[\"predicted_picks\"][:]\n",
    "    array_dist = f[\"array_dist\"][:]\n",
    "    \n",
    "### Read raw and denoised DAS\n",
    "with h5py.File(raw_dir + 'KKFLStill2024_02_24.hdf5', 'r') as f:\n",
    "    raw_quake_kkfls = f[\"raw_quake\"][:, :4500, :] # original could be 500:5000, check \n",
    "    fk_quake_kkfls = f[\"fk_quake\"][:, :4500, :]\n",
    "\n",
    "with h5py.File(raw_dir + 'TERRAtill2024_02_24.hdf5', 'r') as f:\n",
    "    raw_quake_terra = f[\"raw_quake\"][:, :4500, :]\n",
    "    fk_quake_terra = f[\"fk_quake\"][:, :4500, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0fcd3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_quake_kkfls.shape\n",
    "raw_quake_terra.shape\n",
    "\n",
    "#flip the terra data on the channel axis, the number of ea\n",
    "quakes = np.concatenate((raw_quake_kkfls[:, ::-1, :], raw_quake_terra), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff8da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bandpass filter\n",
    "b, a = butter(4, (0.5, 12), fs=sample_rate, btype='bandpass')\n",
    "filt = filtfilt(b, a, quakes, axis=2)\n",
    "rawdata = filt / np.std(filt, axis=(1,2), keepdims=True)  ## Rawdata w.r.t. Denoised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd0a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 9000, 1500)\n",
      "(40, 9000, 1500)\n"
     ]
    }
   ],
   "source": [
    "#import the masks generated by previous notebook\n",
    "p_s_quake_masks = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/p_s_quake_masks_06042025.npy\")\n",
    "p_wave_masks = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/mask_p_waves_06042025.npy\")\n",
    "s_wave_masks = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/mask_s_waves_06042025.npy\")\n",
    "#load the indices files\n",
    "p_wave_indices = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/p_wave_indices_06042025.npy\")\n",
    "s_wave_indices = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/s_wave_indices_06042025.npy\")\n",
    "both_p_s_indices = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/both_p_s_indices_06042025.npy\")\n",
    "\n",
    "#combined the indices into one array\n",
    "combined_indices = np.concatenate((p_wave_indices, s_wave_indices, both_p_s_indices), axis=0)\n",
    "\n",
    "#print(combined_indices.shape)\n",
    "#print(combined_indices)\n",
    "\n",
    "#combined the p_wave masks, s_wave masks and p_s_quake_masks into one array\n",
    "combined_masks = np.concatenate((p_wave_masks, s_wave_masks, p_s_quake_masks), axis=0)\n",
    "\n",
    "#create a for loop that takes only the i values from combined_indices and makes a new rawdata array with the earthquake data indexed in the specific order\n",
    "new_rawdata = rawdata[combined_indices, :, :]\n",
    "\n",
    "#duplicate every odd row and create an even row with the same data\n",
    "new_combined = np.repeat(combined_masks, 2, axis=1)\n",
    "\n",
    "print(new_rawdata.shape)\n",
    "print(new_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "760d05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL FROM (Shi et al., 2025), utilizing datalabel class for denodas_train\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class unet(nn.Module):\n",
    "    def __init__(self, ch_in, ch0, ch_max, factors=None, kernel=(3, 3), pad=(1, 1), use_att=False):\n",
    "        super(unet, self).__init__()\n",
    "        self.level = len(factors)\n",
    "        self.factor = factors\n",
    "        self.relu = nn.ReLU()\n",
    "        self.kernel = kernel\n",
    "        self.pad = pad\n",
    "        self.use_att = use_att\n",
    "        self.layer = nn.ModuleList([])\n",
    "\n",
    "        if self.use_att:\n",
    "            self.attgates = nn.ModuleList([])\n",
    "            for i in range(self.level):\n",
    "                nch = min(ch0 * 2 ** i, ch_max)\n",
    "                self.attgates.append(AttentionGate(nch))\n",
    "\n",
    "        for i in range(self.level + 1):\n",
    "            if i == 0:\n",
    "                nch_input = ch_in\n",
    "            else:\n",
    "                nch_input = nch_output\n",
    "            nch_output = min(ch0 * 2 ** i, ch_max)\n",
    "            self.layer.append(nn.Conv2d(nch_input, nch_output, self.kernel, padding=self.pad))\n",
    "            self.layer.append(nn.Conv2d(nch_output, nch_output, self.kernel, padding=self.pad))\n",
    "            if i > self.level - 2:\n",
    "                self.layer.append(nn.Dropout(p=0.2))\n",
    "            if i < self.level:\n",
    "                self.layer.append(MaxBlurPool2d(nch_output, kernel_size=(self.factor[i], self.factor[i])))\n",
    "\n",
    "        for i in range(self.level):\n",
    "            nch_input = min(ch0 * 2 ** (self.level - i), ch_max)\n",
    "            nch_output = min(ch0 * 2 ** (self.level - i - 1), ch_max)\n",
    "            scale_factor = (self.factor[self.level - 1 - i], self.factor[self.level - 1 - i])\n",
    "            self.layer.append(nn.Upsample(scale_factor=scale_factor, mode='nearest'))\n",
    "            self.layer.append(nn.Conv2d(nch_input, nch_output, self.kernel, padding=self.pad))\n",
    "            self.layer.append(nn.Conv2d(nch_input, nch_output, self.kernel, padding=self.pad))\n",
    "            self.layer.append(nn.Conv2d(nch_output, nch_output, self.kernel, padding=self.pad))\n",
    "\n",
    "        self.layer.append(nn.Conv2d(nch_output, ch_in, self.kernel, padding=self.pad))\n",
    "\n",
    "        # Track decoder layer indices\n",
    "        self.decoder_layer_indices = list(range(3 * self.level + 4, len(self.layer)))\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        cat_content = []\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        for i in range(self.level - 1):\n",
    "            x = self.layer[3 * i + 0](x)\n",
    "            x = self.relu(x)\n",
    "            x = self.layer[3 * i + 1](x)\n",
    "            x = self.relu(x)\n",
    "            cat_content.append(x)\n",
    "            x = self.layer[3 * i + 2](x)\n",
    "\n",
    "        x = self.layer[3 * (self.level - 1) + 0](x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer[3 * (self.level - 1) + 1](x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer[3 * (self.level - 1) + 2](x)\n",
    "        cat_content.append(x)\n",
    "        x = self.layer[3 * (self.level - 1) + 3](x)\n",
    "\n",
    "        x = self.layer[3 * self.level + 1](x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer[3 * self.level + 2](x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer[3 * self.level + 3](x)\n",
    "\n",
    "        st_lvl = 3 * self.level + 4\n",
    "        for i in range(self.level):\n",
    "            x = self.layer[st_lvl + i * 4 + 0](x)\n",
    "            x = self.layer[st_lvl + i * 4 + 1](x)\n",
    "            x = self.relu(x)\n",
    "\n",
    "            if self.use_att:\n",
    "                cat = self.attgates[-1 * (i + 1)](cat_content[-1 * (i + 1)], x)\n",
    "            else:\n",
    "                cat = cat_content[-1 * (i + 1)]\n",
    "            x = torch.cat((cat, x), dim=1)\n",
    "\n",
    "            x = self.layer[st_lvl + i * 4 + 2](x)\n",
    "            x = self.relu(x)\n",
    "            x = self.layer[st_lvl + i * 4 + 3](x)\n",
    "            x = self.relu(x)\n",
    "\n",
    "        x = self.layer[7 * self.level + 4](x)\n",
    "        x = x.squeeze(1)\n",
    "        x = x.squeeze(1)\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def reinit_decoder(self):\n",
    "        for idx in self.decoder_layer_indices:\n",
    "            self.layer[idx].apply(self._reinit_single_layer)\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        encoder_indices = list(range(0, 3 * self.level + 4))\n",
    "        for idx in encoder_indices:\n",
    "            for param in self.layer[idx].parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def _reinit_single_layer(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "def load_weights_and_reset_decoder(model, checkpoint_path):\n",
    "    state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.reinit_decoder()\n",
    "    model.freeze_encoder()\n",
    "\n",
    "\n",
    "\n",
    "class MaxBlurPool2d(nn.Module):\n",
    "    def __init__(self, nch, kernel_size=(2, 2)):\n",
    "        \"\"\" must specify:\n",
    "            Max pool\n",
    "        \"\"\"\n",
    "        super(MaxBlurPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        a = self.gaussion_filter(self.kernel_size[0])\n",
    "        b = self.gaussion_filter(self.kernel_size[1])\n",
    "        f = torch.matmul(a[:, None], b[None, :])\n",
    "\n",
    "        f = f / torch.sum(f)\n",
    "        f = f[None, None, :, :]\n",
    "        f = f.repeat(nch, nch, 1, 1)\n",
    "\n",
    "        pad1 = (kernel_size[0] - 1) // 2\n",
    "        pad2 = kernel_size[0] - 1 - pad1\n",
    "        pad3 = (kernel_size[1] - 1) // 2\n",
    "        pad4 = kernel_size[1] - 1 - pad3\n",
    "        pads = np.array([pad3, pad4, pad1, pad2])\n",
    "        pads = torch.from_numpy(pads)\n",
    "        filter = f.to(dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer('pads', pads)\n",
    "        self.register_buffer('filter', filter)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.MaxPool2d(kernel_size=self.kernel_size)(x)\n",
    "        x = F.pad(x, self.pads.tolist(), 'constant', 0)\n",
    "        x = F.conv2d(x, self.filter, stride=(1, 1), padding='valid')\n",
    "        return x\n",
    "\n",
    "    def gaussion_filter(self, kernel_size):\n",
    "\n",
    "        if kernel_size == 1:\n",
    "            f = torch.tensor([1., ])\n",
    "        elif kernel_size == 2:\n",
    "            f = torch.tensor([1., 1.])\n",
    "        elif kernel_size == 3:\n",
    "            f = torch.tensor([1., 2., 1.])\n",
    "        elif kernel_size == 4:\n",
    "            f = torch.tensor([1., 3., 3., 1.])\n",
    "        elif kernel_size == 5:\n",
    "            f = torch.tensor([1., 4., 6., 4., 1.])\n",
    "        elif kernel_size == 6:\n",
    "            f = torch.tensor([1., 5., 10., 10., 5., 1.])\n",
    "        elif kernel_size == 7:\n",
    "            f = torch.tensor([1., 6., 15., 20., 15., 6., 1.])\n",
    "        return f\n",
    "\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, nch):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(nch, nch, (1, 1), padding=0)\n",
    "        self.conv2 = nn.Conv2d(nch, nch, (1, 1), padding=0)\n",
    "        self.conv3 = nn.Conv2d(nch, nch, (1, 1), padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, enc, dec):\n",
    "        x = self.conv1(enc)\n",
    "        y = self.conv2(dec)\n",
    "        z = self.relu(x+y)\n",
    "        z = self.sigmoid(self.conv3(z))\n",
    "\n",
    "        return enc * z\n",
    "\n",
    "class datalabel(nn.Module):\n",
    "\n",
    "    def __init__(self, X, Y, Nx_sub=1500, stride=750, mask_ratio=0.1, n_masks=10):\n",
    "        \"\"\" This code assumes input size to be Ni, Nx=1500*n, Nt=1500；\n",
    "            extract 1500^2 square samples and do masking in a bootstrap manner\"\"\"\n",
    "\n",
    "        self.X = X  # DAS matrix\n",
    "        self.Y = Y  # DAS matrix\n",
    "        self.Ni = X.shape[0]\n",
    "        self.Nx = X.shape[1]\n",
    "        self.Nt = X.shape[2]\n",
    "        self.Nx_sub = Nx_sub  # Number of channels per sample\n",
    "        self.stride = stride\n",
    "        self.n_masks = n_masks  # number of times repeating the mask\n",
    "        self.mask_traces = int(mask_ratio * Nx_sub)  # the number traces to mask\n",
    "        self.__data_generation()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Number of samples \"\"\"\n",
    "        return int(self.n_masks * self.Ni * ((self.Nx - self.Nx_sub) / self.stride + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.samples[idx], self.labels[idx])\n",
    "\n",
    "    def __data_generation(self):\n",
    "        X = self.X\n",
    "        Y = self.Y\n",
    "        Ni = self.Ni\n",
    "        Nt = self.Nt\n",
    "        Nx = self.Nx\n",
    "        Nx_sub = self.Nx_sub\n",
    "        stride = self.stride\n",
    "        n_masks = self.n_masks\n",
    "        mask_traces = self.mask_traces\n",
    "\n",
    "        n_total = self.__len__()  # total number of samples\n",
    "        samples = np.zeros((n_total, Nx_sub, Nt), dtype=np.float32)\n",
    "        labels = np.zeros((n_total, Nx_sub, Nt), dtype=np.float32)\n",
    "        masks = np.ones_like(samples, dtype=np.float32)\n",
    "        print(samples.shape)\n",
    "\n",
    "        # Loop over samples\n",
    "        for k in range(n_masks):\n",
    "            for i in range(Ni):\n",
    "                for j, st_ch in enumerate(np.arange(0, Nx-Nx_sub+1, stride)):\n",
    "                    # %% slice each big image along channels\n",
    "                    s = (k * Ni + i) * int((Nx-Nx_sub)//stride+1) + j\n",
    "                    samples[s, :, :] = X[i, st_ch:st_ch + Nx_sub, :]\n",
    "                    labels[s, :, :] = Y[i, st_ch:st_ch + Nx_sub, :]\n",
    "\n",
    "                    # rng = np.random.default_rng(s + 11)\n",
    "                    # trace_masked = rng.choice(Nx_sub, size=mask_traces, replace=False)\n",
    "                    # masks[s, trace_masked, :] = masks[s, trace_masked, :] * 0\n",
    "                    \n",
    "                    # del trace_masked, rng\n",
    "                    gc.collect()\n",
    "\n",
    "        self.samples = samples\n",
    "        # self.masks = masks\n",
    "        self.labels = labels\n",
    "        \n",
    "        del X, Y\n",
    "        gc.collect()\n",
    "        \n",
    "        pass\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6adc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model from (Shi et al., 2023), loss_fn function changed from MSEloss to cross entropy loss, also utilized datalabel class\n",
    "#intead of the dataflow as I am utiliing the labeled data.\n",
    "\n",
    "import h5py\n",
    "import time\n",
    "import torch\n",
    "import configparser\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train(filtered_data, picks,configure_file='config.ini'):\n",
    "    # Data\n",
    "    print(\"filtered_data shape:\", filtered_data.shape, \"picks shape:\", picks.shape)\n",
    "\n",
    "    x = filtered_data\n",
    "    y = picks\n",
    "\n",
    "    #normalize the x data\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        x_min = np.min(x[i])\n",
    "        x_max = np.max(x[i])\n",
    "        x[i] = (x[i] - x_min) / (x_max - x_min)\n",
    "\n",
    "    x = np.repeat(x, 2, axis=0)\n",
    "    y = np.repeat(y, 2, axis=0)\n",
    "    #normalize the x data\n",
    "\n",
    "    print(\"x shape:\", x.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    \n",
    "    print(x.shape, y.shape, x.dtype, y.dtype) #batch 1st dimension, then 1500 by 1500\n",
    "\n",
    "    eighty_percent_length = int(x.shape[0] * 0.8)\n",
    "    print(\"before datalabel\")\n",
    "    training_data = datalabel(x[:eighty_percent_length], y[:eighty_percent_length])  # Use datalabel for training data\n",
    "    validation_data = datalabel(x[eighty_percent_length:], y[eighty_percent_length:])  # Use datalabel for validation data\n",
    "\n",
    "    print(\"Attempting to initialize the U-net model...\")\n",
    "    # Initialize the U-net model\n",
    "    model = unet(1, 16, 1024, factors=(5, 3, 2, 2))\n",
    "    print(\"U-net model initialized successfully.\")\n",
    "    devc = try_gpu(i=1)\n",
    "    model.to(devc)\n",
    "\n",
    "\n",
    "    print(\"Post device utilization\")\n",
    "\n",
    "    # Hyper-parameters for training\n",
    "    batch_size = 2\n",
    "    lr = 1e-2\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    train_iter = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    validate_iter = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',        # reduce LR when the monitored quantity has stopped decreasing\n",
    "        factor=0.5,        # reduce by half\n",
    "        patience=3,        # wait 3 epochs before reducing\n",
    "    )\n",
    "\n",
    "    print(\"Start training...\")\n",
    "    # Train the model\n",
    "    model, avg_train_losses, avg_valid_losses = train_augmentation(train_iter,\n",
    "                                                                   validate_iter,\n",
    "                                                                   model,\n",
    "                                                                   loss_fn,\n",
    "                                                                   optimizer,\n",
    "                                                                   lr_scheduler,\n",
    "                                                                   epochs=50,\n",
    "                                                                   patience=6,\n",
    "                                                                   device=devc,\n",
    "                                                                   minimum_epochs=5)\n",
    "\n",
    "\n",
    "def try_gpu(i=0):  # @save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "            trace_func (function): trace print function.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def train_augmentation(train_dataloader, validate_dataloader, model, loss_fn, optimizer, lr_schedule, epochs,\n",
    "                        patience, device, minimum_epochs=None):\n",
    "    # get early_stopping ready\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    # save history of losses every epoch\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "\n",
    "    print(f'Training on {device} ...')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        starttime = time.time()  # record time for each epoch\n",
    "        train_losses = []  # save loss for every batch\n",
    "        valid_losses = []\n",
    "        print(f'Epoch {epoch}/{epochs}')\n",
    "        # ======================= training =======================\n",
    "      #  print(\"training\")\n",
    "        model.train()  # train mode on\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # #change x and y to numpy arrays\n",
    "            # x_np = X.cpu().numpy()\n",
    "            # y_np = y.cpu().numpy()\n",
    "\n",
    "            # fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            # ax[0].imshow(x_np[1], aspect='auto', cmap = \"RdBu\", vmin = -0.5, vmax = 0.5, interpolation = \"none\")\n",
    "            # ax[0].imshow(y_np[1], aspect='auto', cmap = cmap_orange_transparent, alpha=0.5, interpolation = \"none\")\n",
    "\n",
    "            # ax[1].imshow(x_np[1], aspect='auto', cmap = \"RdBu\", vmin = -0.5, vmax = 0.5, interpolation = \"none\")\n",
    "            # ax[1].imshow(y_np[1], aspect='auto', cmap = cmap_orange_transparent, alpha=0.5, interpolation = \"none\")\n",
    "\n",
    "            # plt.show()\n",
    "            # plt.close()\n",
    "\n",
    "            # predict and loss\n",
    "            pred = model(X)\n",
    "            #print()\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ======================= validating =======================\n",
    "      #  print(\"validating\")\n",
    "        model.eval()  # evaluation model on\n",
    "        with torch.no_grad():\n",
    "            for (X, y) in validate_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "               # print(X, y)\n",
    "\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                valid_losses.append(loss.item())\n",
    "        \n",
    "        # average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        lr_schedule.step(valid_loss)\n",
    "\n",
    "        # ==================== history monitoring ====================\n",
    "        \n",
    "      #  print(\"history monitoring\")\n",
    "        # print training/validation statistics\n",
    "        epoch_len = len(str(epochs))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'time per epoch: {(time.time() - starttime):.3f} s')\n",
    "        print(print_msg)\n",
    "\n",
    "        if (minimum_epochs is None) or ((minimum_epochs is not None) and (epoch > minimum_epochs)):\n",
    "            # if the current valid loss is lowest, save a checkpoint of model weights\n",
    "            early_stopping(valid_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # load the last checkpoint as the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return model, avg_train_losses, avg_valid_losses\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78fd8a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_data shape: (2, 9000, 1500) picks shape: (2, 9000, 1500)\n",
      "x shape: (4, 9000, 1500)\n",
      "y shape: (4, 9000, 1500)\n",
      "(4, 9000, 1500) (4, 9000, 1500) float64 uint8\n",
      "before datalabel\n",
      "(330, 1500, 1500)\n",
      "(110, 1500, 1500)\n",
      "Attempting to initialize the U-net model...\n",
      "U-net model initialized successfully.\n",
      "Post device utilization\n",
      "Start training...\n",
      "Training on cuda:1 ...\n",
      "Epoch 1/50\n",
      "[ 1/50] train_loss: 16424663.55168 valid_loss: 1938.34199 time per epoch: 24.641 s\n",
      "Epoch 2/50\n",
      "[ 2/50] train_loss: 2474.42902 valid_loss: 1938.30096 time per epoch: 24.227 s\n",
      "Epoch 3/50\n",
      "[ 3/50] train_loss: 2474.41041 valid_loss: 1938.29070 time per epoch: 24.307 s\n",
      "Epoch 4/50\n",
      "[ 4/50] train_loss: 2474.40465 valid_loss: 1938.28241 time per epoch: 24.213 s\n",
      "Epoch 5/50\n",
      "[ 5/50] train_loss: 2474.40169 valid_loss: 1938.28088 time per epoch: 24.300 s\n",
      "Epoch 6/50\n",
      "[ 6/50] train_loss: 2474.40018 valid_loss: 1938.28048 time per epoch: 24.297 s\n",
      "Validation loss decreased (inf --> 1938.280482).  Saving model ...\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_rawdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(filtered_data, picks, configure_file)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStart training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m model, avg_train_losses, avg_valid_losses = \u001b[43mtrain_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m                                                               \u001b[49m\u001b[43mvalidate_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m                                                               \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m                                                               \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m                                                               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m                                                               \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m                                                               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m                                                               \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m                                                               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m                                                               \u001b[49m\u001b[43mminimum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36mtrain_augmentation\u001b[39m\u001b[34m(train_dataloader, validate_dataloader, model, loss_fn, optimizer, lr_schedule, epochs, patience, device, minimum_epochs)\u001b[39m\n\u001b[32m    179\u001b[39m       optimizer.zero_grad()\n\u001b[32m    180\u001b[39m       loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m       \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m   \u001b[38;5;66;03m# ======================= validating =======================\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m#  print(\"validating\")\u001b[39;00m\n\u001b[32m    185\u001b[39m   model.eval()  \u001b[38;5;66;03m# evaluation model on\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/optim/optimizer.py:63\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_use_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     prev_grad = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     65\u001b[39m         \u001b[38;5;66;03m# Note on graph break below:\u001b[39;00m\n\u001b[32m     66\u001b[39m         \u001b[38;5;66;03m# we need to graph break to ensure that aot respects the no_grad annotation.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# graph break to allow the fully fused fwd-bwd-optimizer graph to be compiled.\u001b[39;00m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# see https://github.com/pytorch/pytorch/issues/104053\u001b[39;00m\n\u001b[32m     77\u001b[39m         torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = train(new_rawdata[:2], new_combined[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736c135",
   "metadata": {},
   "source": [
    "## Debugging Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5992fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = datalabel(new_rawdata[0:2], new_combined[0:2])\n",
    "# test2 = datalabel(new_rawdata[2:4], new_combined[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d7537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 9000, 1500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rawdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "x = new_rawdata[:2]\n",
    "y = new_combined[:2]\n",
    "\n",
    "x = (x - np.mean(x, axis=(1, 2), keepdims=True)) / np.std(x, axis=(1, 2), keepdims=True)\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    x_min = np.min(x[i])\n",
    "    x_max = np.max(x[i])\n",
    "    x[i] = (x[i] - x_min) / (x_max - x_min)\n",
    "    print(np.max(x[i,:,:]))\n",
    "\n",
    "\n",
    "\n",
    "# x = np.repeat(x, 2, axis=0)\n",
    "# y = np.repeat(y, 2, axis=0)\n",
    "# #normalize the x data\n",
    "\n",
    "# print(\"x shape:\", x.shape)\n",
    "# print(\"y shape:\", y.shape)\n",
    "\n",
    "# print(x.shape, y.shape, x.dtype, y.dtype) #batch 1st dimension, then 1500 by 1500\n",
    "\n",
    "# #take 80% of the data total length of x[0] for training and 20% for validation\n",
    "# eighty_percent_length = int(x.shape[0] * 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb08c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(eighty_percent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 1500, 1500)\n",
      "(220, 1500, 1500)\n",
      "(8, 9000, 1500) (8, 9000, 1500) float64 uint8\n"
     ]
    }
   ],
   "source": [
    "training_data = datalabel(x[:eighty_percent_length], y[:eighty_percent_length])  # Use datalabel for training data\n",
    "validation_data = datalabel(x[eighty_percent_length:], y[eighty_percent_length:])\n",
    "\n",
    "print(x.shape, y.shape, x.dtype, y.dtype) #batch 1st dimension, then 1500 by 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 1500, 1500) (660, 1500, 1500)\n",
      "(220, 1500, 1500) (220, 1500, 1500)\n"
     ]
    }
   ],
   "source": [
    "print(training_data.samples.shape, training_data.labels.shape)\n",
    "# Initialize the U-net od\n",
    "print(validation_data.samples.shape, validation_data.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6214cd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fe0cc6c0a10> <torch.utils.data.dataloader.DataLoader object at 0x7fe0cc6c2350>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "train_iter = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validate_iter = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(train_iter, validate_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c192f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 1: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 2: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 3: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 4: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 5: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 6: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 7: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 8: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 9: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 10: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 11: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 12: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 13: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 14: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 15: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 16: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 17: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 18: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 19: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 20: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 21: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 22: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 23: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 24: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 25: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 26: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 27: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 28: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 29: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 30: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 31: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 32: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 33: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 34: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 35: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 36: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 37: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 38: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 39: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 40: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 41: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 42: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 43: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 44: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 45: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 46: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 47: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 48: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 49: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 50: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 51: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 52: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 53: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 54: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 55: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 56: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 57: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 58: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 59: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 60: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 61: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 62: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 63: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 64: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 65: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 66: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 67: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 68: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 69: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 70: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 71: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 72: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 73: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 74: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 75: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 76: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 77: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 78: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 79: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 80: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 81: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 82: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 83: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 84: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 85: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 86: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 87: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 88: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 89: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 90: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 91: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 92: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 93: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 94: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 95: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 96: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 97: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 98: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 99: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 100: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 101: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 102: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 103: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 104: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 105: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 106: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 107: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 108: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 109: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 110: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 111: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 112: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 113: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 114: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 115: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 116: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 117: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 118: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 119: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 120: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 121: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 122: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 123: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 124: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 125: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 126: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 127: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 128: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 129: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 130: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 131: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 132: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 133: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 134: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 135: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 136: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 137: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 138: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 139: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 140: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 141: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 142: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 143: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 144: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 145: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 146: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 147: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 148: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 149: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 150: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 151: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 152: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 153: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 154: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 155: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 156: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 157: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 158: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 159: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 160: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 161: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 162: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 163: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 164: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 165: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 166: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 167: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 168: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 169: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 170: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 171: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 172: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 173: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 174: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 175: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 176: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 177: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 178: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 179: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 180: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 181: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 182: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 183: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 184: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 185: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 186: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 187: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 188: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 189: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 190: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 191: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 192: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 193: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 194: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 195: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 196: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 197: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 198: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 199: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 200: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 201: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 202: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 203: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 204: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 205: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 206: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 207: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 208: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 209: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 210: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 211: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 212: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 213: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 214: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 215: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 216: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 217: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 218: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 219: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 220: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 221: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 222: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 223: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 224: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 225: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 226: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 227: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 228: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 229: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 230: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 231: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 232: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 233: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 234: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 235: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 236: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 237: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 238: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 239: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 240: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 241: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 242: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 243: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 244: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 245: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 246: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 247: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 248: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 249: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 250: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 251: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 252: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 253: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 254: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 255: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 256: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 257: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 258: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 259: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 260: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 261: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 262: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 263: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 264: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 265: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 266: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 267: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 268: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 269: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 270: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 271: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 272: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 273: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 274: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 275: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 276: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 277: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 278: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 279: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 280: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 281: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 282: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 283: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 284: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 285: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 286: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 287: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 288: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 289: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 290: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 291: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 292: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 293: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 294: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 295: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 296: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 297: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 298: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 299: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 300: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 301: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 302: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 303: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 304: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 305: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 306: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 307: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 308: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 309: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 310: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 311: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 312: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 313: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 314: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 315: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 316: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 317: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 318: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 319: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 320: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 321: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 322: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 323: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 324: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 325: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 326: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 327: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 328: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n",
      "Batch 329: X shape: torch.Size([2, 1500, 1500]), y shape: torch.Size([2, 1500, 1500])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(train_iter):\n",
    "    print(f\"Batch {batch}: X shape: {X.shape}, y shape: {y.shape}\")     \n",
    "\n",
    "                # pred = model(X)\n",
    "                # loss = loss_fn(pred, y)\n",
    "                # valid_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caddcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_data shape: (4, 9000, 1500) picks shape: (4, 9000, 1500)\n",
      "x shape: (8, 9000, 1500)\n",
      "y shape: (8, 9000, 1500)\n",
      "(8, 9000, 1500) (8, 9000, 1500) float64 uint8\n",
      "before datalabel\n",
      "(660, 1500, 1500)\n",
      "(220, 1500, 1500)\n",
      "Attempting to initialize the U-net model...\n",
      "1\n",
      "2\n",
      "U-net model initialized successfully.\n",
      "Post device utilization\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_rawdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(filtered_data, picks, configure_file)\u001b[39m\n\u001b[32m     52\u001b[39m train_iter = DataLoader(training_data, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     53\u001b[39m validate_iter = DataLoader(validation_data, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m lr_scheduler = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# reduce LR when the monitored quantity has stopped decreasing\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# reduce by half\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# wait 3 epochs before reducing\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStart training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "model = train(new_rawdata[:4], new_combined[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b868375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
