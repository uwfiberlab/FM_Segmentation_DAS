{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403d9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738cdeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters for DAS \n",
    "sample_rate = 25\n",
    "dchan = 9.5714\n",
    "ch_max = 4500  # max channel of each cable (4500 or 6000)\n",
    "ch_itv=2  # channels are downsampled for faster picking\n",
    "\n",
    "### Directories and files\n",
    "raw_dir = '/fd1/QibinShi_data/akdas/qibin_data/'\n",
    "out_dir = raw_dir + 'largerEQ_plots_test_picking_dec_ch' + str(ch_max) + '/'\n",
    "record_time_file = 'recording_times_larger.csv'\n",
    "qml = raw_dir + 'ak_Dec1_31_a120b065.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942f3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read phase picks from the previous session\n",
    "with h5py.File(out_dir + 'phase_picks.hdf5', 'r') as f: #filepaths should be \n",
    "    raw_picks = f[\"raw_alldata_picks\"][:]\n",
    "    one_picks = f[\"one_denoise_picks\"][:]\n",
    "    mul_picks = f[\"mul_denoise_picks\"][:]\n",
    "    pred_picks = f[\"predicted_picks\"][:]\n",
    "    array_dist = f[\"array_dist\"][:]\n",
    "    \n",
    "### Read raw and denoised DAS\n",
    "with h5py.File(raw_dir + 'KKFLStill2024_02_24.hdf5', 'r') as f:\n",
    "    raw_quake_kkfls = f[\"raw_quake\"][:, :4500, :] # original could be 500:5000, check \n",
    "    fk_quake_kkfls = f[\"fk_quake\"][:, :4500, :]\n",
    "\n",
    "with h5py.File(raw_dir + 'TERRAtill2024_02_24.hdf5', 'r') as f:\n",
    "    raw_quake_terra = f[\"raw_quake\"][:, :4500, :]\n",
    "    fk_quake_terra = f[\"fk_quake\"][:, :4500, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fcd3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_quake_kkfls.shape\n",
    "raw_quake_terra.shape\n",
    "\n",
    "#flip the terra data on the channel axis, the number of ea\n",
    "quakes = np.concatenate((raw_quake_kkfls[:, ::-1, :], raw_quake_terra), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff8da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bandpass filter\n",
    "b, a = butter(4, (0.5, 12), fs=sample_rate, btype='bandpass')\n",
    "filt = filtfilt(b, a, quakes, axis=2)\n",
    "rawdata = filt / np.std(filt, axis=(1,2), keepdims=True)  ## Rawdata w.r.t. Denoised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd0a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 9000, 1500)\n",
      "(40, 9000, 1500)\n"
     ]
    }
   ],
   "source": [
    "#import the masks generated by previous notebook\n",
    "p_s_quake_masks = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/p_s_quake_masks_06042025.npy\")\n",
    "p_wave_masks = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/mask_p_waves_06042025.npy\")\n",
    "s_wave_masks = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/mask_s_waves_06042025.npy\")\n",
    "#load the indices files\n",
    "p_wave_indices = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/p_wave_indices_06042025.npy\")\n",
    "s_wave_indices = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/s_wave_indices_06042025.npy\")\n",
    "both_p_s_indices = np.load(\"/home/arose17/FM_Segmentation_DAS/src/data/pick_masking/cleaned_picks_with_data/alex_verified_06042025/masks/both_p_s_indices_06042025.npy\")\n",
    "\n",
    "#combined the indices into one array\n",
    "combined_indices = np.concatenate((p_wave_indices, s_wave_indices, both_p_s_indices), axis=0)\n",
    "\n",
    "#print(combined_indices.shape)\n",
    "#print(combined_indices)\n",
    "\n",
    "#combined the p_wave masks, s_wave masks and p_s_quake_masks into one array\n",
    "combined_masks = np.concatenate((p_wave_masks, s_wave_masks, p_s_quake_masks), axis=0)\n",
    "\n",
    "#create a for loop that takes only the i values from combined_indices and makes a new rawdata array with the earthquake data indexed in the specific order\n",
    "new_rawdata = rawdata[combined_indices, :, :]\n",
    "\n",
    "#duplicate every odd row and create an even row with the same data\n",
    "new_combined = np.repeat(combined_masks, 2, axis=1)\n",
    "\n",
    "print(new_rawdata.shape)\n",
    "print(new_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "760d05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL FROM (Shi et al., 2025), utilizing datalabel class for denodas_train\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "class unet(nn.Module):\n",
    "    \"\"\" Stack convolutional blocks. \"\"\"\n",
    "\n",
    "    def __init__(self, ch_in, ch0, ch_max, factors=None, kernel=(3, 3), pad=(1, 1), use_att=False):\n",
    "        \"\"\" must specify:\n",
    "            ch_in:   channel number of data\n",
    "            ch0:     channel number of first feature map\n",
    "            ch_max:  maximum number of channels\n",
    "            factors: resample factors of every block\n",
    "        \"\"\"\n",
    "        super(unet, self).__init__()\n",
    "        self.level = len(factors)  # number of blocks downwards/upwards\n",
    "        self.factor = factors\n",
    "        self.relu = nn.ReLU()\n",
    "        self.kernel = kernel\n",
    "        self.pad = pad\n",
    "        self.use_att = use_att\n",
    "        self.layer = nn.ModuleList([])\n",
    "        if self.use_att:\n",
    "            self.attgates = nn.ModuleList([])\n",
    "            for i in range(self.level):\n",
    "                nch = min(ch0 * 2 ** i, ch_max)\n",
    "                self.attgates.append(AttentionGate(nch))\n",
    "\n",
    "        print(\"1\")\n",
    "\n",
    "        \"\"\"Deepening to extract features\"\"\"\n",
    "        for i in range(self.level+1):\n",
    "            # %% an unit level, conv+conv+pool\n",
    "            if i == 0:\n",
    "                nch_input = ch_in\n",
    "            else:\n",
    "                nch_input = nch_output\n",
    "            nch_output = min(ch0 * 2**i, ch_max)\n",
    "            self.layer.append(nn.Conv2d(nch_input, nch_output, self.kernel, padding=self.pad))\n",
    "            self.layer.append(nn.Conv2d(nch_output, nch_output, self.kernel, padding=self.pad))\n",
    "\n",
    "            if i > self.level-2:  # only drop out at the bottom two levels\n",
    "                self.layer.append(nn.Dropout(p=0.2))\n",
    "            if i < self.level:\n",
    "                self.layer.append(MaxBlurPool2d(nch_output, kernel_size=(self.factor[i], self.factor[i])))\n",
    "\n",
    "        \"\"\"Shallowing to reconstruct wavefields\"\"\"\n",
    "        for i in range(self.level):\n",
    "            # %% an unit level, upsample+conv+conv+conv\n",
    "            nch_input = min(ch0 * 2 ** (self.level - i), ch_max)\n",
    "            nch_output = min(ch0 * 2 ** (self.level - i - 1), ch_max)\n",
    "            scale_factor = (self.factor[self.level - 1 - i], self.factor[self.level - 1 - i])\n",
    "            self.layer.append(nn.Upsample(scale_factor=scale_factor, mode='nearest'))\n",
    "            self.layer.append(nn.Conv2d(nch_input, nch_output, self.kernel, padding=self.pad))\n",
    "            # skip-connect will be here\n",
    "            self.layer.append(nn.Conv2d(nch_input, nch_output, self.kernel, padding=self.pad))\n",
    "            self.layer.append(nn.Conv2d(nch_output, nch_output, self.kernel, padding=self.pad))\n",
    "\n",
    "        print(\"2\")\n",
    "\n",
    "        self.layer.append(nn.Conv2d(nch_output, ch_in, self.kernel, padding=self.pad))\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        cat_content = []\n",
    "        \"\"\"Deepening to extract features\"\"\"\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        for i in range(self.level-1):\n",
    "            x = self.layer[3 * i + 0](x)  # conv1\n",
    "            x = self.relu(x)\n",
    "            x = self.layer[3 * i + 1](x)  # conv2\n",
    "            x = self.relu(x)\n",
    "            cat_content.append(x)\n",
    "            x = self.layer[3 * i + 2](x)  # pool\n",
    "\n",
    "        x = self.layer[3 * (self.level - 1) + 0](x)  # conv1\n",
    "        x = self.relu(x)\n",
    "        x = self.layer[3 * (self.level - 1) + 1](x)  # conv2\n",
    "        x = self.relu(x)\n",
    "        x = self.layer[3 * (self.level - 1) + 2](x)  # dropout\n",
    "        cat_content.append(x)\n",
    "        x = self.layer[3 * (self.level - 1) + 3](x)  # pool\n",
    "\n",
    "        x = self.layer[3 * self.level + 1](x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer[3 * self.level + 2](x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer[3 * self.level + 3](x)  # dropout\n",
    "\n",
    "        print(\"3\")\n",
    "\n",
    "        \"\"\"Shallowing to reconstruct wavefields\"\"\"\n",
    "        st_lvl = 3 * self.level + 4  # start from 3level+4\n",
    "        for i in range(self.level):\n",
    "            x = self.layer[st_lvl + i * 4 + 0](x)  # upsample\n",
    "            x = self.layer[st_lvl + i * 4 + 1](x)  # conv1\n",
    "            x = self.relu(x)\n",
    "\n",
    "            if self.use_att:\n",
    "                cat = self.attgates[-1 * (i + 1)](cat_content[-1 * (i + 1)], x)\n",
    "            else:\n",
    "                cat = cat_content[-1 * (i + 1)]\n",
    "            x = torch.cat((cat, x), dim=1)\n",
    "\n",
    "            x = self.layer[st_lvl + i * 4 + 2](x)  # conv2\n",
    "            x = self.relu(x)\n",
    "            x = self.layer[st_lvl + i * 4 + 3](x)  # conv3\n",
    "            x = self.relu(x)\n",
    "            \n",
    "        x = self.layer[7 * self.level + 4](x)  # (None, 1, Nx, Nt) conv4\n",
    "  \n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class MaxBlurPool2d(nn.Module):\n",
    "    def __init__(self, nch, kernel_size=(2, 2)):\n",
    "        \"\"\" must specify:\n",
    "            Max pool\n",
    "        \"\"\"\n",
    "        super(MaxBlurPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        a = self.gaussion_filter(self.kernel_size[0])\n",
    "        b = self.gaussion_filter(self.kernel_size[1])\n",
    "        f = torch.matmul(a[:, None], b[None, :])\n",
    "\n",
    "        f = f / torch.sum(f)\n",
    "        f = f[None, None, :, :]\n",
    "        f = f.repeat(nch, nch, 1, 1)\n",
    "\n",
    "        pad1 = (kernel_size[0] - 1) // 2\n",
    "        pad2 = kernel_size[0] - 1 - pad1\n",
    "        pad3 = (kernel_size[1] - 1) // 2\n",
    "        pad4 = kernel_size[1] - 1 - pad3\n",
    "        pads = np.array([pad3, pad4, pad1, pad2])\n",
    "        pads = torch.from_numpy(pads)\n",
    "        filter = f.to(dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer('pads', pads)\n",
    "        self.register_buffer('filter', filter)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.MaxPool2d(kernel_size=self.kernel_size)(x)\n",
    "        x = F.pad(x, self.pads.tolist(), 'constant', 0)\n",
    "        x = F.conv2d(x, self.filter, stride=(1, 1), padding='valid')\n",
    "        return x\n",
    "\n",
    "    def gaussion_filter(self, kernel_size):\n",
    "\n",
    "        if kernel_size == 1:\n",
    "            f = torch.tensor([1., ])\n",
    "        elif kernel_size == 2:\n",
    "            f = torch.tensor([1., 1.])\n",
    "        elif kernel_size == 3:\n",
    "            f = torch.tensor([1., 2., 1.])\n",
    "        elif kernel_size == 4:\n",
    "            f = torch.tensor([1., 3., 3., 1.])\n",
    "        elif kernel_size == 5:\n",
    "            f = torch.tensor([1., 4., 6., 4., 1.])\n",
    "        elif kernel_size == 6:\n",
    "            f = torch.tensor([1., 5., 10., 10., 5., 1.])\n",
    "        elif kernel_size == 7:\n",
    "            f = torch.tensor([1., 6., 15., 20., 15., 6., 1.])\n",
    "        return f\n",
    "\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, nch):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(nch, nch, (1, 1), padding=0)\n",
    "        self.conv2 = nn.Conv2d(nch, nch, (1, 1), padding=0)\n",
    "        self.conv3 = nn.Conv2d(nch, nch, (1, 1), padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, enc, dec):\n",
    "        x = self.conv1(enc)\n",
    "        y = self.conv2(dec)\n",
    "        z = self.relu(x+y)\n",
    "        z = self.sigmoid(self.conv3(z))\n",
    "\n",
    "        return enc * z\n",
    "\n",
    "class datalabel(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, Y, Nx_sub=1500, stride=750, mask_ratio=0.1, n_masks=10):\n",
    "        \"\"\" This code assumes input size to be Ni, Nx=1500*n, Nt=1500ï¼›\n",
    "            extract 1500^2 square samples and do masking in a bootstrap manner\"\"\"\n",
    "\n",
    "        self.X = X  # DAS matrix\n",
    "        self.Y = Y  # DAS matrix\n",
    "        self.Ni = X.shape[0]\n",
    "        self.Nx = X.shape[1]\n",
    "        self.Nt = X.shape[2]\n",
    "        self.Nx_sub = Nx_sub  # Number of channels per sample\n",
    "        self.stride = stride\n",
    "        self.n_masks = n_masks  # number of times repeating the mask\n",
    "        self.mask_traces = int(mask_ratio * Nx_sub)  # the number traces to mask\n",
    "        self.__data_generation()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Number of samples \"\"\"\n",
    "        return int(self.n_masks * self.Ni * ((self.Nx - self.Nx_sub) / self.stride + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.samples[idx], self.masks[idx])#, self.masked_labels[idx], self.X, self.Y\n",
    "\n",
    "    def __data_generation(self):\n",
    "        X = self.X\n",
    "        Y = self.Y\n",
    "        Ni = self.Ni\n",
    "        Nt = self.Nt\n",
    "        Nx = self.Nx\n",
    "        Nx_sub = self.Nx_sub\n",
    "        stride = self.stride\n",
    "        n_masks = self.n_masks\n",
    "        mask_traces = self.mask_traces\n",
    "\n",
    "        n_total = self.__len__()  # total number of samples\n",
    "        samples = np.zeros((n_total, Nx_sub, Nt), dtype=np.float32)\n",
    "        labels = np.zeros((n_total, Nx_sub, Nt), dtype=np.float32)\n",
    "        masks = np.ones_like(samples, dtype=np.float32)\n",
    "        print(samples.shape)\n",
    "\n",
    "        print(\"datalabel\", X.shape)\n",
    "        print(\"datalabel\", Y.shape)\n",
    "\n",
    "        # Loop over samples\n",
    "        for k in range(n_masks):\n",
    "            for i in range(Ni):\n",
    "                for j, st_ch in enumerate(np.arange(0, Nx-Nx_sub+1, stride)):\n",
    "                    # %% slice each big image along channels\n",
    "                    s = (k * Ni + i) * int((Nx-Nx_sub)//stride+1) + j\n",
    "                    samples[s, :, :] = X[i, st_ch:st_ch + Nx_sub, :]\n",
    "                    labels[s, :, :] = Y[i, st_ch:st_ch + Nx_sub, :]\n",
    "\n",
    "                    rng = np.random.default_rng(s + 11)\n",
    "                    trace_masked = rng.choice(Nx_sub, size=mask_traces, replace=False)\n",
    "                    masks[s, trace_masked, :] = masks[s, trace_masked, :] * 0\n",
    "                    \n",
    "                    del trace_masked, rng\n",
    "                    gc.collect()\n",
    "\n",
    "        self.samples = samples\n",
    "        self.masks = masks\n",
    "        self.masked_labels = labels * (1 - masks)\n",
    "        \n",
    "        del X, Y\n",
    "        gc.collect()\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6adc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model from (Shi et al., 2023), loss_fn function changed from MSEloss to cross entropy loss, also utilized datalabel class\n",
    "#intead of the dataflow as I am utiliing the labeled data.\n",
    "\n",
    "import h5py\n",
    "import time\n",
    "import torch\n",
    "import configparser\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train(filtered_data, picks,configure_file='config.ini'):\n",
    "    # Data\n",
    "    print(\"filtered_data shape:\", filtered_data.shape, \"picks shape:\", picks.shape)\n",
    "\n",
    "    x = filtered_data\n",
    "    y = picks\n",
    "\n",
    "    x = np.repeat(x, 5, axis=0)\n",
    "    y = np.repeat(y, 5, axis=0)\n",
    "    #normalize the x data\n",
    "\n",
    "    print(\"x shape:\", x.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    \n",
    "    print(x.shape, y.shape, x.dtype, y.dtype) #batch 1st dimension, then 1500 by 1500\n",
    "\n",
    "    #take 80% of the data total length of x[0] for training and 20% for validation\n",
    "    eighty_percent_length = int(len(x[0]) * 0.8)\n",
    "\n",
    "    training_data = datalabel(x[:eighty_percent_length], y[:eighty_percent_length])  # Use datalabel for training data\n",
    "    validation_data = datalabel(x[eighty_percent_length:], y[eighty_percent_length:])  # Use datalabel for validation data\n",
    "\n",
    "    print(\"Attempting to initialize the U-net model...\")\n",
    "    # Initialize the U-net model\n",
    "    model = unet(1, 16, 1024, factors=(5, 3, 2, 2))\n",
    "    print(\"U-net model initialized successfully.\")\n",
    "    devc = try_gpu(i=1)\n",
    "    model.to(devc)\n",
    "\n",
    "    # Hyper-parameters for training\n",
    "    batch_size = 10\n",
    "    lr = 1e-3\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    train_iter = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    validate_iter = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',        # reduce LR when the monitored quantity has stopped decreasing\n",
    "        factor=0.5,        # reduce by half\n",
    "        patience=3,        # wait 3 epochs before reducing\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model, avg_train_losses, avg_valid_losses = train_augmentation(train_iter,\n",
    "                                                                   validate_iter,\n",
    "                                                                   model,\n",
    "                                                                   loss_fn,\n",
    "                                                                   optimizer,\n",
    "                                                                   lr_scheduler,\n",
    "                                                                   epochs=50,\n",
    "                                                                   patience=6,\n",
    "                                                                   device=devc,\n",
    "                                                                   minimum_epochs=5)\n",
    "\n",
    "\n",
    "def try_gpu(i=0):  # @save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "            trace_func (function): trace print function.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "def train_augmentation(train_dataloader, validate_dataloader, model, loss_fn, optimizer, lr_schedule, epochs,\n",
    "                        patience, device, minimum_epochs=None):\n",
    "    # get early_stopping ready\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    # save history of losses every epoch\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "\n",
    "    print(f'Training on {device} ...')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        starttime = time.time()  # record time for each epoch\n",
    "        train_losses = []  # save loss for every batch\n",
    "        valid_losses = []\n",
    "\n",
    "        # ======================= training =======================\n",
    "        model.train()  # train mode on\n",
    "        for batch, ((X, mask), y) in enumerate(train_dataloader):\n",
    "            X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "\n",
    "            # predict and loss\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ======================= validating =======================\n",
    "        model.eval()  # evaluation model on\n",
    "        with torch.no_grad():\n",
    "            for (X, mask), y in validate_dataloader:\n",
    "                X, mask, y = X.to(device), mask.to(device), y.to(device)\n",
    "\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "        # average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        lr_schedule.step(valid_loss)\n",
    "\n",
    "        # ==================== history monitoring ====================\n",
    "        # print training/validation statistics\n",
    "        epoch_len = len(str(epochs))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'time per epoch: {(time.time() - starttime):.3f} s')\n",
    "        print(print_msg)\n",
    "\n",
    "        if (minimum_epochs is None) or ((minimum_epochs is not None) and (epoch > minimum_epochs)):\n",
    "            # if the current valid loss is lowest, save a checkpoint of model weights\n",
    "            early_stopping(valid_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # load the last checkpoint as the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return model, avg_train_losses, avg_valid_losses\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5992fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = datalabel(new_rawdata[0:2], new_combined[0:2])\n",
    "# test2 = datalabel(new_rawdata[2:4], new_combined[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89d7537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 9000, 1500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rawdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_data shape: (40, 9000, 1500) picks shape: (40, 9000, 1500)\n",
      "x shape: (200, 9000, 1500)\n",
      "y shape: (200, 9000, 1500)\n",
      "(200, 9000, 1500) (200, 9000, 1500) float64 uint8\n",
      "(22000, 1500, 1500)\n",
      "datalabel (200, 9000, 1500)\n",
      "datalabel (200, 9000, 1500)\n"
     ]
    }
   ],
   "source": [
    "model = train(new_rawdata, new_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
